#!/bin/bash

# LightOnOCR llama-server ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ë¡œê³  ì¶œë ¥
echo -e "${BLUE}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘        LightOnOCR Server v1.0         â•‘"
echo "â•‘     Powered by llama.cpp & MPS        â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

# ì„¤ì •
MODELS_DIR="./models"
MODEL_FILE="$MODELS_DIR/LightOnOCR-1B-1025-Q8_0.gguf"
MMPROJ_FILE="$MODELS_DIR/mmproj-LightOnOCR-1B-1025-Q8_0.gguf"
CONTEXT_SIZE=8192
GPU_LAYERS=999  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPU(MPS)ë¡œ
HOST="0.0.0.0"
PORT=8080
THREADS=-1  # ìžë™ ê°ì§€

# ëª¨ë¸ íŒŒì¼ ì¡´ìž¬ í™•ì¸
if [ ! -f "$MODEL_FILE" ]; then
    echo -e "${RED}âœ— ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤${NC}"
    echo "   ìœ„ì¹˜: $MODEL_FILE"
    echo ""
    echo "   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:"
    echo "   ./download_models.sh"
    exit 1
fi

if [ ! -f "$MMPROJ_FILE" ]; then
    echo -e "${YELLOW}âš  ë©€í‹°ëª¨ë‹¬ í”„ë¡œì ì…˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤${NC}"
    echo "   ìœ„ì¹˜: $MMPROJ_FILE"
    echo "   OCR ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤"
fi

echo "ðŸ”§ ì„œë²„ ì„¤ì •:"
echo "   ëª¨ë¸: $(basename "$MODEL_FILE")"
echo "   í”„ë¡œì ì…˜: $(basename "$MMPROJ_FILE")"
echo "   ëª¨ë¸ í¬ê¸°: $(du -h "$MODEL_FILE" | cut -f1)"
echo "   ì»¨í…ìŠ¤íŠ¸: $CONTEXT_SIZE í† í°"
echo "   GPU ë ˆì´ì–´: $GPU_LAYERS (MPS ê°€ì†)"
echo "   ì£¼ì†Œ: http://$HOST:$PORT"
echo ""

# llama-server ì¡´ìž¬ í™•ì¸
if ! command -v llama-server &> /dev/null; then
    echo -e "${RED}âœ— llama-serverë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤${NC}"
    echo "  ./setup_macos.shë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”"
    exit 1
fi

# ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ í™•ì¸
if lsof -i :$PORT &> /dev/null; then
    echo -e "${YELLOW}âš  í¬íŠ¸ $PORTê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ìž…ë‹ˆë‹¤${NC}"
    echo "  ê¸°ì¡´ ì„œë²„ë¥¼ ì¢…ë£Œí•˜ê±°ë‚˜ ë‹¤ë¥¸ í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
    echo "  ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ: kill \$(lsof -t -i:$PORT)"
    exit 1
fi

echo -e "${GREEN}ðŸš€ ì„œë²„ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...${NC}"
echo "   ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•œ ê²½ìš° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤ (ì•½ 2GB)"
echo ""
echo "ðŸ“ ì‚¬ìš©ë²•:"
echo "   1. ì›¹ UI: http://localhost:$PORT"
echo "   2. API í…ŒìŠ¤íŠ¸: python test_ocr.py"
echo "   3. ì¢…ë£Œ: Ctrl+C"
echo ""
echo "========================================="
echo ""

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs

# í˜„ìž¬ ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
LOG_FILE="logs/llama_server_$(date +%Y%m%d_%H%M%S).log"

echo "ðŸ“„ ë¡œê·¸ íŒŒì¼: $LOG_FILE"
echo ""

# llama-server ì‹¤í–‰
# -m: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
# --mmproj: ë©€í‹°ëª¨ë‹¬ í”„ë¡œì ì…˜ íŒŒì¼
# -c: ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
# -ngl: GPU ë ˆì´ì–´ ìˆ˜ (MPS ê°€ì†)
# --host: ë°”ì¸ë“œ ì£¼ì†Œ
# --port: í¬íŠ¸
# -t: ìŠ¤ë ˆë“œ ìˆ˜
# --parallel: ë™ì‹œ ìš”ì²­ ìŠ¬ë¡¯ ìˆ˜
# --ubatch-size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
exec llama-server \
    -m "$MODEL_FILE" \
    --mmproj "$MMPROJ_FILE" \
    -c $CONTEXT_SIZE \
    -ngl $GPU_LAYERS \
    --host $HOST \
    --port $PORT \
    -t $THREADS \
    --parallel 4 \
    --ubatch-size 2048 \
    2>&1 | tee "$LOG_FILE"