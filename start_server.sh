#!/bin/bash

# LightOnOCR llama-server ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ë¡œê³  ì¶œë ¥
echo -e "${BLUE}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘        LightOnOCR Server v1.0         â•‘"
echo "â•‘     Powered by llama.cpp & MPS        â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

# ì„¤ì •
MODEL="ggml-org/LightOnOCR-1B-1025-GGUF"
CONTEXT_SIZE=8192
GPU_LAYERS=999  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPU(MPS)ë¡œ
HOST="0.0.0.0"
PORT=8080
THREADS=-1  # ìžë™ ê°ì§€

echo "ðŸ”§ ì„œë²„ ì„¤ì •:"
echo "   ëª¨ë¸: $MODEL"
echo "   ì»¨í…ìŠ¤íŠ¸: $CONTEXT_SIZE í† í°"
echo "   GPU ë ˆì´ì–´: $GPU_LAYERS (MPS ê°€ì†)"
echo "   ì£¼ì†Œ: http://$HOST:$PORT"
echo ""

# llama-server ì¡´ìž¬ í™•ì¸
if ! command -v llama-server &> /dev/null; then
    echo -e "${RED}âœ— llama-serverë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤${NC}"
    echo "  ./setup_macos.shë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”"
    exit 1
fi

# ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ í™•ì¸
if lsof -i :$PORT &> /dev/null; then
    echo -e "${YELLOW}âš  í¬íŠ¸ $PORTê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ìž…ë‹ˆë‹¤${NC}"
    echo "  ê¸°ì¡´ ì„œë²„ë¥¼ ì¢…ë£Œí•˜ê±°ë‚˜ ë‹¤ë¥¸ í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
    echo "  ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ: kill \$(lsof -t -i:$PORT)"
    exit 1
fi

echo -e "${GREEN}ðŸš€ ì„œë²„ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...${NC}"
echo "   ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•œ ê²½ìš° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤ (ì•½ 2GB)"
echo ""
echo "ðŸ“ ì‚¬ìš©ë²•:"
echo "   1. ì›¹ UI: http://localhost:$PORT"
echo "   2. API í…ŒìŠ¤íŠ¸: python test_ocr.py"
echo "   3. ì¢…ë£Œ: Ctrl+C"
echo ""
echo "========================================="
echo ""

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs

# í˜„ìž¬ ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
LOG_FILE="logs/llama_server_$(date +%Y%m%d_%H%M%S).log"

echo "ðŸ“„ ë¡œê·¸ íŒŒì¼: $LOG_FILE"
echo ""

# llama-server ì‹¤í–‰
# -hf: Hugging Face ëª¨ë¸ ì§ì ‘ ë¡œë“œ
# -c: ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
# -ngl: GPU ë ˆì´ì–´ ìˆ˜ (MPS ê°€ì†)
# --host: ë°”ì¸ë“œ ì£¼ì†Œ
# --port: í¬íŠ¸
# -t: ìŠ¤ë ˆë“œ ìˆ˜
exec llama-server \
    -hf "$MODEL" \
    -c $CONTEXT_SIZE \
    -ngl $GPU_LAYERS \
    --host $HOST \
    --port $PORT \
    -t $THREADS \
    2>&1 | tee "$LOG_FILE"